{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import dspy\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cambridge_dictionary():\n",
    "    data = None\n",
    "    with open(\"../data/cambridge/cambridge.word.json\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mw_xml_to_dict(soup, start_word, num_word, stop_word):\n",
    "  \"\"\"\n",
    "  Get example sentences from the webster dictionary\n",
    "  input:\n",
    "    soup: BeautifulSoup object\n",
    "    start_word: the word we want to start with\n",
    "    num_word: number of words we want to get\n",
    "    stop_word: the word we want to stop at\n",
    "  output:\n",
    "    output_dict: a dictionary contains the word and its example sentences\n",
    "  \"\"\"\n",
    "  pattern = r\"{ldquo}|{rdquo}|{it}|{phrase}|{/phrase}|{dx}|{/dx}|\\[\\=.*\\]|\\n\"\n",
    "  output_dict = {}\n",
    "  word_count = 0\n",
    "  find = False\n",
    "  # Find all words that contain example sentence/phrase\n",
    "  for word in soup.find_all('entry'):\n",
    "    # Get the number of words we want\n",
    "    if (stop_word != None and word.find('id').text == stop_word):\n",
    "      break\n",
    "    elif (find and word_count == num_word):\n",
    "      break\n",
    "\n",
    "    # First, find the target word\n",
    "    word_id = word.find('id').text\n",
    "    if (not find) and (word_id != start_word):\n",
    "      continue\n",
    "    else:\n",
    "      find = True\n",
    "\n",
    "    # Find each definition of POS of the word\n",
    "    pos = word.find_all('fl')\n",
    "    big_sense_list = []\n",
    "    output_dict[word_id] = {}\n",
    "    for ele in pos:\n",
    "      fl = ele.contents[0]\n",
    "      output_dict[word_id][fl] = []\n",
    "      pos_list = []\n",
    "\n",
    "      definition = ele.find_next('def')  # big sense\n",
    "      big_sense_list = []\n",
    "      # Find senses for each definition\n",
    "      senses = definition.find_all('sense')\n",
    "\n",
    "\n",
    "      for content in senses:\n",
    "        sense_dict = {}\n",
    "        sense = content.find('dt')\n",
    "        if sense.contents[0] == '\\n':\n",
    "          sense = sense.find('un').contents[0]\n",
    "          sense = re.sub(pattern, \"\", sense)\n",
    "        else:\n",
    "          sense = sense.contents[0]\n",
    "          sense = re.sub(pattern, \"\", sense)\n",
    "        sense_dict[\"en_def\"] = sense\n",
    "\n",
    "\n",
    "        # Get example sentences\n",
    "        examples = content.find_all('vi')\n",
    "        exam_list = []\n",
    "        if examples != []:\n",
    "          for sentence in examples:\n",
    "            example = re.sub(pattern, \"\", sentence.text)\n",
    "            example = example.replace(r\"{/it}\", \"\")\n",
    "            exam_dict = {\"en\": example}\n",
    "            exam_list.append(exam_dict)\n",
    "        sense_dict[\"examples\"] = exam_list\n",
    "\n",
    "        big_sense_list.append({\"sense\": [sense_dict]})\n",
    "      output_dict[word_id][fl].append({\"big_sense\": big_sense_list})\n",
    "\n",
    "    word_count += 1\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_webster_dictionary(start_word, stop_word=None, num_word=0):\n",
    "    file = None\n",
    "    with open(f\"../data/mw/mw-xml/LD_u.xml\", \"r\") as f:\n",
    "      file = f.read()\n",
    "    soup = BeautifulSoup(file, 'xml')\n",
    "    output_dict = convert_mw_xml_to_dict(soup, start_word, num_word, stop_word)\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cam_dict \u001b[38;5;241m=\u001b[39m load_cambridge_dictionary()\n\u001b[0;32m----> 2\u001b[0m web_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_webster_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mugly\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munderstand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mload_webster_dictionary\u001b[0;34m(start_word, stop_word, num_word)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/mw/mw-xml/LD_u.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m   file \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 5\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m convert_mw_xml_to_dict(soup, start_word, num_word, stop_word)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_dict\n",
      "File \u001b[0;32m~/EMI-linggle-search/env_lg/lib/python3.10/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/EMI-linggle-search/env_lg/lib/python3.10/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/EMI-linggle-search/env_lg/lib/python3.10/site-packages/bs4/builder/_lxml.py:237\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    235\u001b[0m         data \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCHUNK_SIZE)\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 237\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m, etree\u001b[38;5;241m.\u001b[39mParserError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:1331\u001b[0m, in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:1451\u001b[0m, in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parsertarget.pxi:161\u001b[0m, in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parsertarget.pxi:156\u001b[0m, in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/etree.pyx:351\u001b[0m, in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/saxparser.pxi:576\u001b[0m, in \u001b[0;36mlxml.etree._handleSaxData\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parsertarget.pxi:108\u001b[0m, in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxData\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/EMI-linggle-search/env_lg/lib/python3.10/site-packages/bs4/builder/_lxml.py:345\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.data\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_data(data)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mendData(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_instruction_class)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m(\u001b[38;5;28mself\u001b[39m, content):\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_data(content)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdoctype\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, pubid, system):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cam_dict = load_cambridge_dictionary()\n",
    "web_dict = load_webster_dictionary(\"ugly\", \"understand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/mw/mw-json/mw.word.json\", \"w\") as f:\n",
    "    json.dump(web_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain target word and its senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_and_sense(word, dictionary, word_sense):\n",
    "    \"\"\"\n",
    "    Get the word and its sense from the dictionary\n",
    "    input:\n",
    "      word: the word we want to find\n",
    "      dictionary: the dictionary that contains the word\n",
    "      word_sense: a dictionary contains the word and its sense\n",
    "    output:\n",
    "      word_sense: a dictionary contains the word and its sense\n",
    "    \"\"\"\n",
    "    senses = []\n",
    "    for pos in dictionary[word]:\n",
    "        for big_sense_list in dictionary[word][pos]:\n",
    "            for big_sense in big_sense_list[\"big_sense\"]:\n",
    "                for sense in big_sense[\"sense\"]:\n",
    "                    senses.append(sense[\"en_def\"])\n",
    "    word_sense[word] = senses\n",
    "\n",
    "    return word_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ugly': ['unpleasant to look at; not attractive', 'unpleasant and threatening or violent']} {'ugly': ['{bc}unpleasant to look at {bc}not pretty or attractive ', '{bc}unpleasant to hear ', '{bc}offensive or disgusting ', '{bc}very bad or unpleasant ', 'see {dxt|head:1||}']}\n"
     ]
    }
   ],
   "source": [
    "word_sense_cam = {}\n",
    "word_sense_web = {}\n",
    "word_sense_cam = get_word_and_sense(\"ugly\", cam_dict, word_sense_cam)\n",
    "word_sense_web = get_word_and_sense(\"ugly\", web_dict, word_sense_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ugly': ['unpleasant to look at; not attractive',\n",
       "  'unpleasant and threatening or violent']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sense_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Senses(BaseModel):\n",
    "    english_sense: str\n",
    "    chinese_word: str\n",
    "\n",
    "class TranslatedSense(BaseModel):\n",
    "    word: list[Senses]\n",
    "\n",
    "def chat(target_word, word_dict):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"\n",
    "         You are a senior translator specializing in English to Traditional Mandarin Chinese.\n",
    "         Task:\n",
    "         Given an English word along with its definition. Each definition is divided by comma.\n",
    "         Provide one precise and nuanced translation only for each definition.\n",
    "         \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "         Word: {target_word},\n",
    "         Definition: {word_dict[target_word]}\n",
    "         \"\"\"}\n",
    "    ],\n",
    "    response_format = TranslatedSense,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "camb_translations = chat(\"ugly\", word_sense_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"word\":[{\"english_sense\":\"unpleasant to look at; not '\n",
      " 'attractive\",\"chinese_word\":\"醜陋\"},{\"english_sense\":\"unpleasant and '\n",
      " 'threatening or violent\",\"chinese_word\":\"可怕\"}]}')\n"
     ]
    }
   ],
   "source": [
    "pprint(camb_translations.content, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_translation = chat(\"ugly\", word_sense_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"word\":[{\"english_sense\":\"unpleasant to look at, not pretty or '\n",
      " 'attractive\",\"chinese_word\":\"醜陋\"},{\"english_sense\":\"unpleasant to '\n",
      " 'hear\",\"chinese_word\":\"刺耳\"},{\"english_sense\":\"offensive or '\n",
      " 'disgusting\",\"chinese_word\":\"令人厭惡\"},{\"english_sense\":\"very bad or '\n",
      " 'unpleasant\",\"chinese_word\":\"糟糕\"}]}')\n"
     ]
    }
   ],
   "source": [
    "pprint(web_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "gpt4o_mini = dspy.OpenAI(\"gpt-4o-mini\", max_tokens=300)\n",
    "dspy.configure(lm=gpt4o_mini)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
